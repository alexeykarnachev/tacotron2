  # TODO: Is it a good idea to replace hparams from yaml with something like bash-script + args?
  #
  ################################
  # Experiment Parameters        #
  ################################
  # Schedule and seed
  epochs: 500
  iters_per_checkpoint: 1000
  seed: 228
  save_top_k: 3

  # Gpu settings
  fp16_opt_level:
  precision: 32
  gpus:

  # Model settings
  ignore_layers: ['embedding.weight']
  model_class_name: Tacotron2

  ################################
  # Data Parameters              #
  ################################
  load_mel_from_disk: False
  data_directory: /Users/egortimofeev/Documents/data/audio/dummy
  tokenizer_class_name: RussianPhonemeTokenizer

  # Audio preprocessors
  audio_preprocessors:
    SilenceTrimmer:
      top_db: 30
    AmplitudeNormalizer:

  ################################
  # Audio Parameters             #
  ################################
  max_wav_value: 32768.0
  sampling_rate: 16000
  filter_length: 1024
  hop_length: 256
  win_length: 1024
  n_mel_channels: 80
  mel_fmin: 0.0
  mel_fmax: 8000.0

  ################################
  # Model Parameters             #
  ################################
  symbols_embedding_dim: 512

  # Encoder parameters
  encoder_kernel_size: 5
  encoder_n_convolutions: 3
  encoder_embedding_dim: 512

  # Decoder parameters
  n_frames_per_step: 1  # currently only 1 is supported
  decoder_rnn_dim: 1024
  prenet_dim: 256
  max_decoder_steps: 1000
  gate_threshold: 0.5
  p_attention_dropout: 0.1
  p_decoder_dropout: 0.1
  free_running_rate:
    default: 0.1
    '10': 0.2
    '50': 0.3
    '100': 0.5

  # Attention parameters
  attention_rnn_dim: 1024
  attention_dim: 128

  # Location Layer parameters
  attention_location_n_filters: 32
  attention_location_kernel_size: 31

  # Mel-post processing network parameters
  postnet_embedding_dim: 512
  postnet_kernel_size: 5
  postnet_n_convolutions: 5

  ################################
  # Optimization Hyperparameters #
  ################################

  # Learning rate
  warmup_steps: 1000
  learning_rate: 1e-3
  lr_reduce_patience: 5
  lr_reduce_factor: 0.25

  # Weights
  weight_decay: 1e-6
  grad_clip_thresh: 1.0

  # Fit settings
  batch_size: 2
  accum_steps: 1
  mask_padding: true
